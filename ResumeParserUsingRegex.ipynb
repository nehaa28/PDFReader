{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume_file_path = 'D:/my resume/DL_resume.pdf'\n",
    "resume_file_path = 'D:/my resume/Resume-Nehaa.pdf'\n",
    "\n",
    "filehandle = open(resume_file_path,'rb')\n",
    "#rb-read as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader = pdf.PdfFileReader(filehandle)\n",
    "pageHandle = pdfReader.getPage(0)\n",
    "text = pageHandle.extractText()\n",
    "text = text.replace('o ','')\n",
    "text = text.replace('| ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='D:/my resume/Resume-Nehaa.pdf'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filehandle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader = pdf.PdfFileReader(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfReader.getIsEncrypted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfReader.getNumPages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=pdfReader.getPage(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading tika-1.24.tar.gz (28 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tika) (45.2.0.post20200210)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from tika) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (1.25.8)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32888 sha256=3821c321d6579d1f2e0989b5133ae118f7ca3f321e87e3a2df26ae94d767b80c\n",
      "  Stored in directory: c:\\users\\nehaa.bansal\\appdata\\local\\pip\\cache\\wheels\\ec\\2b\\38\\58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-1.24\n"
     ]
    }
   ],
   "source": [
    "!pip install tika\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-18 18:19:48,666 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to C:\\Users\\nehaa.bansal\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2021-06-18 18:20:24,353 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to C:\\Users\\nehaa.bansal\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2021-06-18 18:20:28,840 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "White and Black Minimalist Resume\n",
      "\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Analysis and implementation for obtaining all the universe (Issuer, Equity,\n",
      "Debt and Listing) data for the research purpose.\n",
      "Incorporating the data model (raw, normalized and consolidated) changes\n",
      "as per the requirements \n",
      "Writing normalization, derivation and arbitration logic and validation rules\n",
      "for the vendor data models.\n",
      "Exploring and visualizing data to drive insights.\n",
      "Experienced in feature selection.\n",
      "Advance knowledge of ML algorithms such as Ensemble Models, Ada-\n",
      "boost, Gradient-Boosting, Kernel-KNN, Grid search, Random Forest,\n",
      "Decision Tree, SVM, Linear Regression, Ridge, Lasso, Pipeline, Feature\n",
      "Union, Cross Validation.\n",
      "Hyper-parameter tuning to improve the model Validation metric for\n",
      "Regression and classification algorithms including classification report\n",
      "Participating in constant learning through training and skills development.\n",
      "Worked on Data Annotation Tools like labelmg, label studio etc.\n",
      "Experience in OCR tools like tesseract, EasyOCR etc.\n",
      "Knowledge of NLP based algorithms - TFIDF, Word2Vec, LDA.\n",
      "\n",
      "ML Engineer\n",
      "\n",
      "UIET-Hoshiiarpur, Punjab University\n",
      "B.E.(hons.)- Computer Science Engineering \n",
      "\n",
      "MRA Sr. Sec School, Chandigarh\n",
      "10+2 - 91.6%\n",
      "\n",
      "Bhartiya Vidya Bhavan, Panchkula\n",
      "10 - 95%\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "EDUCATION HIGHLIGHTS\n",
      "\n",
      " - Programming Language: Python\n",
      " - Database : MongoDB, SQL\n",
      " - ML & DL : NumPy, pandas, scikit-learn, TensorFlow, Keras, PyTorch\n",
      " - Visualisation Tool : Matplotlib, Seaborn, plotly, PowerBI, Tableau\n",
      " - IDE : Jupyter Notebook, PyCharm\n",
      " - Deployment Platform & Tool : AWS, GCP, Heroku, Docker\n",
      " - OS : Linux, Windows\n",
      "\n",
      "Contact No: +91-9620871840\n",
      "Email id : nehaa.bansal94@gmail.com         \n",
      "Linkedin : https://www.linkedin.com/in/nehaa-bansal-280494/\n",
      "GitHub: https://github.com/nehaa28\n",
      "\n",
      "NEHAA BANSAL\n",
      "ASSOCIATE CONSULTANT\n",
      "\n",
      "Experience Data Science\n",
      "Engineer skilled in Python,\n",
      "Machine Learning, Data\n",
      "Science, Deep learning. Very\n",
      "enthusiastic about learning\n",
      "and implementing the latest\n",
      "trends in diverse fields of data\n",
      "science. 3 years experience in\n",
      "research and implementation\n",
      "experience in machine\n",
      "learning, deep learning,\n",
      "including regression,\n",
      "classification, neural network,\n",
      "object detection, NLP etc.\n",
      "\n",
      "EXECUTIVE SUMMARY\n",
      "\n",
      "Finance\n",
      "Healthcare\n",
      "Insurance\n",
      "Telecom\n",
      "Banking\n",
      "\n",
      "DOMAINS\n",
      "\n",
      "https://www.linkedin.com/in/nehaa-bansal-280494/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tika import parser # pip install tika\n",
    "\n",
    "raw = parser.from_file('D:/my resume/Resume-Nehaa.pdf')\n",
    "print(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C O N T A C T   M E   A T\n",
      "\n",
      "Address: Katni Madhya pradesh  \n",
      "India\n",
      "\n",
      "avinashmourya05@gmail.com\n",
      "\n",
      "https://github.com/avinash-\n",
      "mourya\n",
      "\n",
      "https://www.linkedin.com/in/a\n",
      "vinash-mourya-05/\n",
      "\n",
      "8718979530\n",
      "\n",
      "S K I L L S\n",
      "\n",
      "Python\n",
      "\n",
      "C++\n",
      "\n",
      "Javascript\n",
      "\n",
      "Data Structure\n",
      "\n",
      "Django\n",
      "\n",
      "T E C H N O L O G I E S  \n",
      "\n",
      "Machine learning\n",
      "Deep learning (ANN, CNN, RNN) \n",
      "Computer Vision (Faster R-CNN,\n",
      "SSD, YOLO) \n",
      "Natural language processing\n",
      "Web development\n",
      "\n",
      "L I B R A R I E S\n",
      "\n",
      "Numpy , Pandas , Matplotlib ,\n",
      "Seaborn , Scikit-learn ,Tensorflow,\n",
      "Keras ,OpenCV ,NLTK, spacy.\n",
      "\n",
      "CERTIFICATIONS\n",
      "\n",
      "Deep Learning Masters\n",
      "\n",
      "AVINASH MOURYA\n",
      "\n",
      "A B O U T   M E\n",
      "\n",
      "A passionate data scientist having knowledge in predictive\n",
      "modeling, machine learning algorithms, exploratory data\n",
      "analysis, feature engineering, and various neural network in\n",
      "deep learning. and focused on deep learning and python\n",
      "programming language.\n",
      "\n",
      "W O R K   E X P E R I E N C E\n",
      "\n",
      "Internship\n",
      "iNeuron  | jan 2021 - April 2021\n",
      "\n",
      "worked on computer vision projects.\n",
      "in this internship, I have worked on object detection,\n",
      "object tracking.\n",
      "\n",
      "Training\n",
      "iNeuron | July 2020 - April 2021\n",
      "\n",
      "Attended 6 Months Training on “Deep learning with\n",
      "computer vision and Advance NLP” \n",
      "Company:- iNeuron ( https://ineuron.ai/ ) \n",
      "Location:- Bengaluru, India. \n",
      "\n",
      "E D U C A T I O N\n",
      "\n",
      "Saraswati Shishu mandir high secondary school Katni\n",
      "Madhya Pradesh \n",
      "   10th | June 2014-April 2015\n",
      "\n",
      "Saraswati Shishu mandir high secondary school Katni\n",
      "Madhya Pradesh \n",
      "\n",
      "12th | June 2016-April 2017\n",
      "\n",
      "Natural Language Processing Masters\n",
      "\n",
      "Samrat Ashok Technological Institute Vidisha, \n",
      "Madhya Pradesh \n",
      "\n",
      "Internship\n",
      "\n",
      "B-tech | Aug 2018-Pursuing\n",
      "\n",
      "Department: Computer Science and Engineering\n",
      "CGPA: 7.48\n",
      "\n",
      "   online course - iNeuron   online course - iNeuron computer vision \f",
      "Traffic objects detection\n",
      "\n",
      "P R O J E C T S\n",
      "https://www.linkedin.com/in/a\n",
      "vinash-mourya-05/\n",
      "\n",
      "involved in building an AI system that detects, track, count and estimate speed of the vehicle.\n",
      "it will help to analyze traffic and 24x7 hours monitor traffic and speed of vehicles.\n",
      "\n",
      "       (Python, Pytorch, yolov5, Deepsort, OpenCV)\n",
      "\n",
      "Facial Recognition\n",
      "\n",
      "Involved in building an AI system that recognizes the face.\n",
      " it will help to take attendance of employees.\n",
      "\n",
      "     (Python, OpenCV, Tensorflow, MTCNN, InsightFace, deepsort)\n",
      "\n",
      "Hand detection\n",
      "https://github.com/avinash-mourya/Hand_detection\n",
      "\n",
      "Involved in building an AI system that detects human hands and tracks the distance between\n",
      "hands and safety line. and when hands cross the safety line it will generate an alert alarm.\n",
      " It will help to prevent an accident in the shredder machine So that the laborers do not lose\n",
      "https://www.linkedin.com/in/a\n",
      "vinash-mourya-05/\n",
      "their hands. \n",
      "\n",
      "https://www.linkedin.com/in/a\n",
      "vinash-mourya-05/\n",
      "\n",
      "      (Python, OpenCV, Tensorflow, Tensorflow Object detection API)\n",
      "\n",
      "Automatic Number plate Recognition \n",
      "https://github.com/avinash-mourya/ANPR\n",
      "\n",
      "I have built an API that detects the Number plate in cars and crop detected number plate and\n",
      "using OCR extract number in the crop number plate. \n",
      "     (python, Django, Tensorflow, OpenCV, tesseract-OCR)\n",
      "\n",
      "Face mask detection \n",
      "https://github.com/avinash-mourya/Face_mask_detection \n",
      "\n",
      "I have built an AI system that detects Face with a mask or without a mask. \n",
      "it will help to identify the health worker and office employees wearing a mask or not.\n",
      "\n",
      "       (Python, OpenCV, Tensorflow)\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "# example_01.py\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf('D:/my resume/DL_resume.pdf'))  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Linkedin', 'Machine Learning', 'Data Science', 'Issuer', 'Debt', 'Random Forest', 'Decision Tree', 'Linear Regression', 'Ridge', 'Lasso', 'Feature Union', 'Cross Validation', 'Data Annotation Tools', 'Punjab University', 'Bhartiya Vidya Bhavan', 'Language', 'Python', 'Keras', 'Matplotlib', 'Jupyter Notebook', 'Tool', 'Heroku', 'Linux', 'Windows']\n"
     ]
    }
   ],
   "source": [
    "#EXTRACTING NAMES\n",
    "# example_04.py\n",
    "\n",
    "#import docx2txt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "'''\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf('D:/my resume/DL_resume.pdf'))  # noqa: T001\n",
    "'''\n",
    "\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "    \n",
    "    return person_names\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('D:/my resume/Resume-Nehaa.pdf')\n",
    "    names = extract_names(text)\n",
    "    if names:\n",
    "        print(names)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8718979530\n"
     ]
    }
   ],
   "source": [
    "# example_05.py\n",
    "\n",
    "import re\n",
    "import subprocess  # noqa: S404\n",
    "\n",
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "\n",
    "\n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('D:/my resume/DL_resume.pdf')\n",
    "    phone_number = extract_phone_number(text)\n",
    "\n",
    "    print(phone_number)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avinashmourya05@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# example_06.py\n",
    "\n",
    "import re\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('D:/my resume/DL_resume.pdf')\n",
    "    emails = extract_emails(text)\n",
    "\n",
    "    if emails:\n",
    "        print(emails[0])  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Machine learning', 'Python', 'python', 'machine learning'}\n"
     ]
    }
   ],
   "source": [
    "# example_07.py\n",
    "\n",
    "#import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('D:/my resume/DL_resume.pdf')\n",
    "    skills = extract_skills(text)\n",
    "\n",
    "    print(skills)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nehaa.bansal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# example_09.py\n",
    "\n",
    "#import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "RESERVED_WORDS = [\n",
    "    'School',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    "\n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "\n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word) >= 0:\n",
    "                education.add(org)\n",
    "\n",
    "    return education\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('D:/my resume/DL_resume.pdf')\n",
    "    education_information = extract_education(text)\n",
    "\n",
    "    print(education_information)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
